{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+B77QuzC9UKH3MrZ9QzVZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luishpinto/text-classification/blob/master/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gLgztEDNP6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  import github\n",
        "except:\n",
        "  !pip install PyGithub\n",
        "  import github"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u22HrvVjNojJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZH6-6glKM7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd06Euea67sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-DY-7AmN3H8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = github.Github('luishpinto','eigHt33n')\n",
        "repo = g.get_user().get_repo('text-classification')\n",
        "files = np.array([],dtype = str)\n",
        "for i in repo.get_contents('/data-base'):\n",
        "  files = np.append(files,i.name)\n",
        "\n",
        "for i in files:\n",
        "  if not os.path.exists(i):\n",
        "    !wget -O {i} 'https://raw.githubusercontent.com/luishpinto/text-classification/master/data-base/{i}'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdUnUefFPVtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flist = dict()\n",
        "flist['yelp'] = 'yelp_labelled.txt'\n",
        "flist['imdb'] = 'imdb_labelled.txt'\n",
        "flist['amazon'] = 'amazon_cells_labelled.txt'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hASeFT-AQ1l0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "22ae7747-2c25-417e-e0cc-6ce27b8406b7"
      },
      "source": [
        "dflist = []\n",
        "for source,path in flist.items():\n",
        "  df = pd.read_csv(path,names = ['sentence','label'],sep = '\\t')\n",
        "  df['source'] = source\n",
        "  dflist.append(df)\n",
        "\n",
        "df = pd.concat(dflist)\n",
        "print(df.iloc[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence    Wow... Loved this place.\n",
            "label                              1\n",
            "source                          yelp\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMPdDBJT7FrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f47d752-186c-480b-83f8-0cf622b0f650"
      },
      "source": [
        "sentences = ['John likes ice cream','John hates chocolate.']\n",
        "vectorizer = CountVectorizer(min_df = 0,lowercase = False)\n",
        "vectorizer.fit(sentences)\n",
        "vectorizer.vocabulary_"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'John': 0, 'chocolate': 1, 'cream': 2, 'hates': 3, 'ice': 4, 'likes': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu0cEYd47dPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ada921ea-b83c-4c58-cf7c-63b15f7e25ca"
      },
      "source": [
        "vectorizer.transform(sentences).toarray()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 1, 1],\n",
              "       [1, 1, 0, 1, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfh4PD8T7sFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "08c33aa2-a60e-4850-88a7-cc20afae49b2"
      },
      "source": [
        "for i in df['source'].unique():\n",
        "  dfyelp = df[df['source'] == i]\n",
        "  X = dfyelp['sentence'].values\n",
        "  y = dfyelp['label'].values\n",
        "\n",
        "  Strain,Stest,ytrain,ytest = train_test_split(X,y,test_size = 0.25,random_state = 1000)\n",
        "\n",
        "  vectorizer = CountVectorizer()\n",
        "  vectorizer.fit(Strain)\n",
        "\n",
        "  Xtrain = vectorizer.transform(Strain)\n",
        "  Xtest = vectorizer.transform(Stest)\n",
        "\n",
        "  clf = LogisticRegression().fit(Xtrain,ytrain)\n",
        "  score = clf.score(Xtest,ytest)\n",
        "\n",
        "  print('Accuracy for {} dataset: {:.3f}'.format(i,score))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for yelp dataset: 0.796\n",
            "Accuracy for imdb dataset: 0.749\n",
            "Accuracy for amazon dataset: 0.796\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}