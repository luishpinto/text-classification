{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZFi9hOoz05igZgp/xceP4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luishpinto/text-classification/blob/master/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhzEVbyKzN4U"
      },
      "source": [
        "# **Text Classification with Python and Scikit-Learn**\n",
        "\n",
        "by Luis H PINTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJBjJ3SVzz8x"
      },
      "source": [
        "The 1st step to be considered is adding the libraries with the necessary packages and functions to the **Python** routines.\n",
        "\n",
        "1. Importing **github** makes possible to import files from **GitHub** repositories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb9OxE9nz05q",
        "outputId": "14f1d967-c370-48fc-a4ef-d0748882da85"
      },
      "source": [
        "try:\n",
        "  !rm -fr './text-classification'\n",
        "  !git clone 'https://github.com/luishpinto/text-classification'\n",
        "except:\n",
        "  !git clone 'https://github.com/luishpinto/text-classification'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'text-classification'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 55 (delta 17), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (55/55), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO857XS71J-p"
      },
      "source": [
        "2. Both **Numpy** and **Pandas** are default library to be used in generic maths and dataframe manipulation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZH6-6glKM7g"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv2f-DuL10wL"
      },
      "source": [
        "3. The **Scikit-Learn** or simply **sklearn** libraries are the core of the **Machine Learning** process. In this notebook a sort of functions and regression models are used.\n",
        "\n",
        "> The functions:\n",
        "\n",
        "> * **CountVectorizer** -- convert a collection of text documents to a matrix of token counts;\n",
        "\n",
        "> * **train_test_split** -- split arrays or matrices into random train and test subsets;\n",
        "\n",
        "> * **Pipeline** -- sequentially apply a list of transforms and a final estimator.\n",
        "\n",
        "> The regression models:\n",
        "\n",
        "> * **LogisticRegression** -- logistic regression classifier;\n",
        "\n",
        "> * **SGDClassifier** -- linear classifiers with stochastic gradient descent (SGD) training.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd06Euea67sv"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXpne5Af4d3H"
      },
      "source": [
        "The 2nd step is related to the data organization.\n",
        "\n",
        "1. Transfering the data-files from the **GitHub** respository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE2Aft_e0mCY"
      },
      "source": [
        "files = !ls -d -1 './text-classification/data-base/'*"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPX3j5_FvX_s"
      },
      "source": [
        "2. Creating the **dataframe**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdUnUefFPVtP"
      },
      "source": [
        "flist = dict()\n",
        "flist['yelp'] = files[0]\n",
        "flist['imdb'] = files[1]\n",
        "flist['amazon'] = files[2]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hASeFT-AQ1l0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed698bf-a766-414f-e89c-d5dfafee7458"
      },
      "source": [
        "dflist = []\n",
        "for source,path in flist.items():\n",
        "  df = pd.read_csv(path,names = ['sentence','label'],sep = '\\t')\n",
        "  df['source'] = source\n",
        "  dflist.append(df)\n",
        "\n",
        "df = pd.concat(dflist)\n",
        "\n",
        "print(df.iloc[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence    So there is no way for me to plug it in here i...\n",
            "label                                                       0\n",
            "source                                                   yelp\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhPrYY_EXi-3"
      },
      "source": [
        "4. The input for any **Regression** or **Classifier** model must be a numeric matrix, so it is necessary to transform sentences in matrix. This operation is performed by the **CountVectorizer** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMPdDBJT7FrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c43897a-9786-4393-f9f1-810a9e04fbd0"
      },
      "source": [
        "sentences = ['John likes ice cream','John hates chocolate.']\n",
        "\n",
        "vectorizer = CountVectorizer(min_df = 0,lowercase = False)\n",
        "vectorizer.fit(sentences)\n",
        "\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'John': 0, 'likes': 5, 'ice': 4, 'cream': 2, 'hates': 3, 'chocolate': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu0cEYd47dPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d40a21d4-bdef-4e17-fb90-402b4713d87b"
      },
      "source": [
        "print(vectorizer.transform(sentences).toarray())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 1 0 1 1]\n",
            " [1 1 0 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoZDjvI7ZEcp"
      },
      "source": [
        "5. Testing the accuracity of the **LogisticRegression**, **DecisionTreeClassifier** and **SGFClassifier**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHEamtTi2Pp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0842b236-f635-44ee-cf73-92568ee5fec7"
      },
      "source": [
        "classifiers = [LogisticRegression(),\n",
        "               DecisionTreeClassifier(),\n",
        "               SGDClassifier()]\n",
        "\n",
        "for clf in classifiers:\n",
        "\n",
        "  print('\\n\\nClassifier: {}\\n'.format(clf))\n",
        "\n",
        "  for i in df['source'].unique():\n",
        "    dfsource = df[df['source'] == i]\n",
        "    S = dfsource['sentence'].values\n",
        "    y = dfsource['label'].values\n",
        "\n",
        "    Strain,Stest,ytrain,ytest = train_test_split(S,y,test_size = 0.25,random_state = 1000)\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(Strain)\n",
        "\n",
        "    Xtrain = vectorizer.transform(Strain)\n",
        "    Xtest = vectorizer.transform(Stest)\n",
        "\n",
        "    pipe = Pipeline(steps = [('classifier',clf)])\n",
        "    pipe.fit(Xtrain,ytrain)\n",
        "    score = pipe.score(Xtest,ytest)\n",
        "\n",
        "    print('Accuracy for {} dataset: {:.3f}'.format(i,score))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Classifier: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "\n",
            "Accuracy for yelp dataset: 0.796\n",
            "Accuracy for imdb dataset: 0.749\n",
            "Accuracy for amazon dataset: 0.796\n",
            "\n",
            "\n",
            "Classifier: DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')\n",
            "\n",
            "Accuracy for yelp dataset: 0.792\n",
            "Accuracy for imdb dataset: 0.626\n",
            "Accuracy for amazon dataset: 0.764\n",
            "\n",
            "\n",
            "Classifier: SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
            "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "\n",
            "Accuracy for yelp dataset: 0.800\n",
            "Accuracy for imdb dataset: 0.727\n",
            "Accuracy for amazon dataset: 0.780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71XnUBk-VJkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24cd1ee1-863c-422e-8509-d484737f997c"
      },
      "source": [
        "mood = ['Bad mood','Good mood']\n",
        "\n",
        "## enter the sentence\n",
        "sentence = \"Crust is no good\"\n",
        "\n",
        "print(mood[pipe.predict(vectorizer.transform([sentence]))[0]])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad mood\n"
          ]
        }
      ]
    }
  ]
}