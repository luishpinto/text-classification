{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvf2gX1ss6A2j1bqWfZlZp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luishpinto/text-classification/blob/master/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gLgztEDNP6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "2cd81717-55ae-42c7-d40c-0c41527477b4"
      },
      "source": [
        "try:\n",
        "  import github\n",
        "except:\n",
        "  !pip install PyGithub\n",
        "  import github"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyGithub\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/87/8ce81695f851697d4112f4c0ce43346b7b0ca52ea6b9e81e65bfedb82f4a/PyGithub-1.53-py3-none-any.whl (274kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 13.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.6/dist-packages (from PyGithub) (2.23.0)\n",
            "Collecting pyjwt\n",
            "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
            "Collecting deprecated\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (1.24.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated->PyGithub) (1.12.1)\n",
            "Installing collected packages: pyjwt, deprecated, PyGithub\n",
            "Successfully installed PyGithub-1.53 deprecated-1.2.10 pyjwt-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u22HrvVjNojJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZH6-6glKM7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd06Euea67sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-DY-7AmN3H8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "caa17b7a-fd29-4b98-97d3-16b69cd75434"
      },
      "source": [
        "g = github.Github(login_or_token = '02c09bdc2afe2369a9db0c98e53668760e87095e')\n",
        "repo = g.get_user().get_repo('text-classification')\n",
        "files = np.array([],dtype = str)\n",
        "for i in repo.get_contents('/data-base'):\n",
        "  files = np.append(files,i.name)\n",
        "\n",
        "for i in files:\n",
        "  if not os.path.exists(i):\n",
        "    !wget -O {i} 'https://raw.githubusercontent.com/luishpinto/text-classification/master/data-base/{i}'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-26 19:28:52--  https://raw.githubusercontent.com/luishpinto/text-classification/master/data-base/amazon_cells_labelled.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58226 (57K) [text/plain]\n",
            "Saving to: ‘amazon_cells_labelled.txt’\n",
            "\n",
            "\r          amazon_ce   0%[                    ]       0  --.-KB/s               \ramazon_cells_labell 100%[===================>]  56.86K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-08-26 19:28:52 (2.07 MB/s) - ‘amazon_cells_labelled.txt’ saved [58226/58226]\n",
            "\n",
            "--2020-08-26 19:28:52--  https://raw.githubusercontent.com/luishpinto/text-classification/master/data-base/imdb_labelled.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85285 (83K) [text/plain]\n",
            "Saving to: ‘imdb_labelled.txt’\n",
            "\n",
            "imdb_labelled.txt   100%[===================>]  83.29K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-08-26 19:28:52 (2.68 MB/s) - ‘imdb_labelled.txt’ saved [85285/85285]\n",
            "\n",
            "--2020-08-26 19:28:52--  https://raw.githubusercontent.com/luishpinto/text-classification/master/data-base/yelp_labelled.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 61320 (60K) [text/plain]\n",
            "Saving to: ‘yelp_labelled.txt’\n",
            "\n",
            "yelp_labelled.txt   100%[===================>]  59.88K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-08-26 19:28:52 (2.13 MB/s) - ‘yelp_labelled.txt’ saved [61320/61320]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdUnUefFPVtP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7a1dd47-18a5-405f-de9c-143a60986ac7"
      },
      "source": [
        "flist = dict()\n",
        "flist['yelp'] = 'yelp_labelled.txt'\n",
        "flist['imdb'] = 'imdb_labelled.txt'\n",
        "flist['amazon'] = 'amazon_cells_labelled.txt'\n",
        "\n",
        "print(flist)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'yelp': 'yelp_labelled.txt', 'imdb': 'imdb_labelled.txt', 'amazon': 'amazon_cells_labelled.txt'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hASeFT-AQ1l0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b77d8722-1a94-443b-b1c3-76864aa8b41e"
      },
      "source": [
        "dflist = []\n",
        "for source,path in flist.items():\n",
        "  df = pd.read_csv(path,names = ['sentence','label'],sep = '\\t')\n",
        "  df['source'] = source\n",
        "  dflist.append(df)\n",
        "\n",
        "df = pd.concat(dflist)\n",
        "\n",
        "print(df.iloc[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence    Wow... Loved this place.\n",
            "label                              1\n",
            "source                          yelp\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMPdDBJT7FrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f86c454-b721-4e62-98b9-865442ab35cc"
      },
      "source": [
        "sentences = ['John likes ice cream','John hates chocolate.']\n",
        "\n",
        "vectorizer = CountVectorizer(min_df = 0,lowercase = False)\n",
        "vectorizer.fit(sentences)\n",
        "\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'John': 0, 'likes': 5, 'ice': 4, 'cream': 2, 'hates': 3, 'chocolate': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu0cEYd47dPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b3bf79f-9774-4483-83ed-de18d6a3d2dc"
      },
      "source": [
        "print(vectorizer.transform(sentences).toarray())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 1 0 1 1]\n",
            " [1 1 0 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHEamtTi2Pp4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "36d47597-239f-48e3-ad36-0f09e40f0e5f"
      },
      "source": [
        "classifiers = [LogisticRegression(),\n",
        "               DecisionTreeClassifier(),\n",
        "               SGDClassifier()]\n",
        "\n",
        "for clf in classifiers:\n",
        "\n",
        "  print('\\n\\nClassifier: {}\\n'.format(clf))\n",
        "\n",
        "  for i in df['source'].unique():\n",
        "    dfsource = df[df['source'] == i]\n",
        "    S = dfsource['sentence'].values\n",
        "    y = dfsource['label'].values\n",
        "\n",
        "    Strain,Stest,ytrain,ytest = train_test_split(S,y,test_size = 0.25,random_state = 1000)\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(Strain)\n",
        "\n",
        "    Xtrain = vectorizer.transform(Strain)\n",
        "    Xtest = vectorizer.transform(Stest)\n",
        "\n",
        "    pipe = Pipeline(steps = [('classifier',clf)])\n",
        "    pipe.fit(Xtrain,ytrain)\n",
        "    score = pipe.score(Xtest,ytest)\n",
        "\n",
        "    print('Accuracy for {} dataset: {:.3f}'.format(i,score))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Classifier: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "\n",
            "Accuracy for yelp dataset: 0.796\n",
            "Accuracy for imdb dataset: 0.749\n",
            "Accuracy for amazon dataset: 0.796\n",
            "\n",
            "\n",
            "Classifier: DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')\n",
            "\n",
            "Accuracy for yelp dataset: 0.768\n",
            "Accuracy for imdb dataset: 0.615\n",
            "Accuracy for amazon dataset: 0.792\n",
            "\n",
            "\n",
            "Classifier: SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
            "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "\n",
            "Accuracy for yelp dataset: 0.800\n",
            "Accuracy for imdb dataset: 0.679\n",
            "Accuracy for amazon dataset: 0.804\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}