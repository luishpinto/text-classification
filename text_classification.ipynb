{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6aAgvHVP/XW14aunqhQik",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luishpinto/text-classification/blob/master/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gLgztEDNP6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  import github\n",
        "except:\n",
        "  !pip install PyGithub\n",
        "  import github"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u22HrvVjNojJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZH6-6glKM7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd06Euea67sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-DY-7AmN3H8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = github.Github(login_or_token = '5aa6a1eb0ddc712b1ca835a113de2ff4e41c6d80')\n",
        "repo = g.get_user().get_repo('text-classification')\n",
        "files = np.array([],dtype = str)\n",
        "for i in repo.get_contents('/data-base'):\n",
        "  files = np.append(files,i.name)\n",
        "\n",
        "for i in files:\n",
        "  if not os.path.exists(i):\n",
        "    !wget -O {i} 'https://raw.githubusercontent.com/luishpinto/text-classification/master/data-base/{i}'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdUnUefFPVtP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f282ec2b-ee6e-4562-cb1f-04b233fbc546"
      },
      "source": [
        "flist = dict()\n",
        "flist['yelp'] = 'yelp_labelled.txt'\n",
        "flist['imdb'] = 'imdb_labelled.txt'\n",
        "flist['amazon'] = 'amazon_cells_labelled.txt'\n",
        "\n",
        "print(flist)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'yelp': 'yelp_labelled.txt', 'imdb': 'imdb_labelled.txt', 'amazon': 'amazon_cells_labelled.txt'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hASeFT-AQ1l0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "09dff00a-535f-4f48-9eeb-e8c9c1b04ad9"
      },
      "source": [
        "dflist = []\n",
        "for source,path in flist.items():\n",
        "  df = pd.read_csv(path,names = ['sentence','label'],sep = '\\t')\n",
        "  df['source'] = source\n",
        "  dflist.append(df)\n",
        "\n",
        "df = pd.concat(dflist)\n",
        "\n",
        "print(df.iloc[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence    Wow... Loved this place.\n",
            "label                              1\n",
            "source                          yelp\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMPdDBJT7FrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b08d128-77e1-4348-d7ca-40aef024438e"
      },
      "source": [
        "sentences = ['John likes ice cream','John hates chocolate.']\n",
        "\n",
        "vectorizer = CountVectorizer(min_df = 0,lowercase = False)\n",
        "vectorizer.fit(sentences)\n",
        "\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'John': 0, 'likes': 5, 'ice': 4, 'cream': 2, 'hates': 3, 'chocolate': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu0cEYd47dPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "101f7492-61b1-4dac-99ad-c41f72b9c875"
      },
      "source": [
        "print(vectorizer.transform(sentences).toarray())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 1 0 1 1]\n",
            " [1 1 0 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHEamtTi2Pp4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "b46c87bb-e3fc-476a-e7df-a07166b41911"
      },
      "source": [
        "classifiers = [LogisticRegression(),\n",
        "               DecisionTreeClassifier(),\n",
        "               SGDClassifier()]\n",
        "\n",
        "for clf in classifiers:\n",
        "\n",
        "  print('\\n\\nClassifier: {}\\n'.format(clf))\n",
        "\n",
        "  for i in df['source'].unique():\n",
        "    dfsource = df[df['source'] == i]\n",
        "    S = dfsource['sentence'].values\n",
        "    y = dfsource['label'].values\n",
        "\n",
        "    Strain,Stest,ytrain,ytest = train_test_split(S,y,test_size = 0.25,random_state = 1000)\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(Strain)\n",
        "\n",
        "    Xtrain = vectorizer.transform(Strain)\n",
        "    Xtest = vectorizer.transform(Stest)\n",
        "\n",
        "    pipe = Pipeline(steps = [('classifier',clf)])\n",
        "    pipe.fit(Xtrain,ytrain)\n",
        "    score = pipe.score(Xtest,ytest)\n",
        "\n",
        "    print('Accuracy for {} dataset: {:.3f}'.format(i,score))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Classifier: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "\n",
            "Accuracy for yelp dataset: 0.796\n",
            "Accuracy for imdb dataset: 0.749\n",
            "Accuracy for amazon dataset: 0.796\n",
            "\n",
            "\n",
            "Classifier: DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')\n",
            "\n",
            "Accuracy for yelp dataset: 0.752\n",
            "Accuracy for imdb dataset: 0.647\n",
            "Accuracy for amazon dataset: 0.800\n",
            "\n",
            "\n",
            "Classifier: SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
            "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "\n",
            "Accuracy for yelp dataset: 0.800\n",
            "Accuracy for imdb dataset: 0.743\n",
            "Accuracy for amazon dataset: 0.792\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}